{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 03 — Pipeline E2E: Decomposer → Planner → Retriever → Grader\n",
    "\n",
    "Run the first 4 nodes of the S4/S6 pipeline on a claim and inspect inputs/outputs at each step.\n",
    "\n",
    "**Nodes covered:**\n",
    "1. **Claim Decomposer** (function) — entities, PICO, sub-claims\n",
    "2. **Retrieval Planner** (ReAct agent) — method selection per sub-claim\n",
    "3. **Evidence Retriever** (ReAct agent) — multi-source search, reranking, dedup\n",
    "4. **Evidence Grader** (ReAct agent) — study type, methodology, relevance, GRADE\n",
    "\n",
    "Each step shows the state delta — what the node added to the pipeline state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from src.models import FactCheckState\n",
    "from src.functions.decomposer import run_decomposer\n",
    "from systems.s4_langgraph.agents.retrieval_planner import run_retrieval_planner\n",
    "from systems.s4_langgraph.agents.evidence_retriever import run_evidence_retriever\n",
    "from systems.s4_langgraph.agents.evidence_grader import run_evidence_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a claim to test\n",
    "CLAIM = \"The MMR vaccine causes autism in children\"\n",
    "\n",
    "# Build initial pipeline state\n",
    "state: FactCheckState = {\n",
    "    \"claim\": CLAIM,\n",
    "    \"pico\": None,\n",
    "    \"sub_claims\": [],\n",
    "    \"entities\": {},\n",
    "    \"retrieval_plan\": {},\n",
    "    \"evidence\": [],\n",
    "    \"extracted_figures\": [],\n",
    "    \"evidence_quality\": {},\n",
    "    \"verdict\": \"\",\n",
    "    \"confidence\": 0.0,\n",
    "    \"explanation\": \"\",\n",
    "    \"safety_flags\": [],\n",
    "    \"is_dangerous\": False,\n",
    "    \"agent_trace\": [],\n",
    "    \"total_cost_usd\": 0.0,\n",
    "    \"total_duration_seconds\": 0.0,\n",
    "}\n",
    "\n",
    "print(f'Claim: \"{CLAIM}\"')\n",
    "print(f'State keys: {list(state.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Claim Decomposer (function node)\n",
    "\n",
    "**What it does:** Extracts medical entities via scispaCy NER, extracts PICO elements, decomposes the claim into atomic sub-claims.\n",
    "\n",
    "**Inputs:** `claim` (string)  \n",
    "**Outputs:** `entities`, `pico`, `sub_claims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await run_decomposer(state)\n",
    "\n",
    "# --- Entities ---\n",
    "print(\"ENTITIES\")\n",
    "print(\"=\" * 50)\n",
    "entities = state[\"entities\"]\n",
    "for etype, elist in entities.items():\n",
    "    if elist:\n",
    "        print(f\"  {etype}: {elist}\")\n",
    "if not any(v for v in entities.values()):\n",
    "    print(\"  (none detected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PICO ---\n",
    "print(\"PICO EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "pico = state[\"pico\"]\n",
    "if pico:\n",
    "    pico_df = pd.DataFrame([{\n",
    "        \"Element\": el,\n",
    "        \"Value\": getattr(pico, el.lower()) or \"(null)\",\n",
    "    } for el in [\"Population\", \"Intervention\", \"Comparison\", \"Outcome\"]])\n",
    "    display(pico_df)\n",
    "else:\n",
    "    print(\"  (no PICO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sub-claims ---\n",
    "print(\"SUB-CLAIMS\")\n",
    "print(\"=\" * 50)\n",
    "sc_rows = []\n",
    "for sc in state[\"sub_claims\"]:\n",
    "    row = {\"id\": sc.id, \"text\": sc.text}\n",
    "    if sc.pico:\n",
    "        row[\"P\"] = sc.pico.population or \"\"\n",
    "        row[\"I\"] = sc.pico.intervention or \"\"\n",
    "        row[\"C\"] = sc.pico.comparison or \"\"\n",
    "        row[\"O\"] = sc.pico.outcome or \"\"\n",
    "    sc_rows.append(row)\n",
    "\n",
    "display(pd.DataFrame(sc_rows))\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "print(f\"\\nDuration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Retrieval Planner (ReAct agent)\n",
    "\n",
    "**What it does:** Examines each sub-claim's characteristics and decides which retrieval methods to use.\n",
    "\n",
    "**Inputs:** `sub_claims`, `entities`, `pico`  \n",
    "**Outputs:** `retrieval_plan` (dict mapping sub-claim ID → list of methods)\n",
    "\n",
    "Uses ReAct with Claude if `ANTHROPIC_API_KEY` is set, otherwise falls back to rule-based keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await run_retrieval_planner(state)\n",
    "\n",
    "print(\"RETRIEVAL PLAN\")\n",
    "print(\"=\" * 60)\n",
    "plan_rows = []\n",
    "for sc_id, methods in state[\"retrieval_plan\"].items():\n",
    "    sc_text = next((sc.text for sc in state[\"sub_claims\"] if sc.id == sc_id), \"?\")\n",
    "    plan_rows.append({\n",
    "        \"sub_claim\": f\"{sc_id}: {sc_text[:60]}\",\n",
    "        \"methods\": \", \".join(methods),\n",
    "        \"count\": len(methods),\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(plan_rows))\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"\\nMode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Evidence Retriever (ReAct agent)\n",
    "\n",
    "**What it does:** Executes the retrieval plan — searches PubMed, Semantic Scholar, Cochrane, ClinicalTrials.gov, DrugBank. Re-ranks with cross-encoder, deduplicates, links evidence to sub-claims.\n",
    "\n",
    "**Inputs:** `retrieval_plan`, `sub_claims`, `pico`, `entities`  \n",
    "**Outputs:** `evidence` (list of Evidence objects), updated `sub_claims` (with evidence IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await run_evidence_retriever(state)\n",
    "\n",
    "evidence = state[\"evidence\"]\n",
    "print(f\"EVIDENCE RETRIEVED: {len(evidence)} items\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Per sub-claim summary\n",
    "for sc in state[\"sub_claims\"]:\n",
    "    sc_ev = [e for e in evidence if e.id in sc.evidence]\n",
    "    by_source = {}\n",
    "    for ev in sc_ev:\n",
    "        by_source[ev.source] = by_source.get(ev.source, 0) + 1\n",
    "    source_str = \", \".join(f\"{s}: {c}\" for s, c in sorted(by_source.items())) if by_source else \"none\"\n",
    "    print(f\"  [{sc.id}] {sc.text[:55]}\")\n",
    "    print(f\"    {len(sc_ev)} items — {source_str}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"\\nMode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all evidence items as a table\n",
    "ev_rows = []\n",
    "for ev in sorted(evidence, key=lambda e: e.quality_score, reverse=True):\n",
    "    ev_rows.append({\n",
    "        \"id\": ev.id,\n",
    "        \"source\": ev.source,\n",
    "        \"study_type\": ev.study_type or \"unknown\",\n",
    "        \"quality_score\": round(ev.quality_score, 3),\n",
    "        \"title\": ev.title[:65],\n",
    "        \"pmid\": ev.pmid or \"\",\n",
    "    })\n",
    "\n",
    "ev_df = pd.DataFrame(ev_rows)\n",
    "print(f\"All evidence items (sorted by cross-encoder quality score):\")\n",
    "display(ev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single evidence item in detail\n",
    "if evidence:\n",
    "    top_ev = sorted(evidence, key=lambda e: e.quality_score, reverse=True)[0]\n",
    "    print(f\"TOP EVIDENCE ITEM: {top_ev.id}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Source:       {top_ev.source}\")\n",
    "    print(f\"  Study type:   {top_ev.study_type}\")\n",
    "    print(f\"  Quality:      {top_ev.quality_score:.4f}\")\n",
    "    print(f\"  PMID:         {top_ev.pmid or '—'}\")\n",
    "    print(f\"  Title:        {top_ev.title}\")\n",
    "    print(f\"  URL:          {top_ev.url or '—'}\")\n",
    "    print(f\"\\n  Content (first 500 chars):\")\n",
    "    print(f\"  {top_ev.content[:500]}\")\n",
    "else:\n",
    "    print(\"No evidence retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Evidence Grader (ReAct agent)\n",
    "\n",
    "**What it does:** Evaluates each evidence item for study type, methodological quality, relevance to sub-claims, and applies the GRADE framework. Produces per-evidence quality scores and per-subclaim evidence summaries.\n",
    "\n",
    "**Inputs:** `evidence`, `sub_claims`, `extracted_figures`  \n",
    "**Outputs:** `evidence_quality` dict with:\n",
    "- `per_evidence`: study type, hierarchy weight, methodology score, relevance, GRADE, evidence strength\n",
    "- `per_subclaim`: evidence count, avg strength, direction summary, top evidence IDs\n",
    "\n",
    "Uses ReAct with Claude if `ANTHROPIC_API_KEY` is set, otherwise falls back to rule-based (uses existing metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await run_evidence_grader(state)\n",
    "\n",
    "eq = state[\"evidence_quality\"]\n",
    "per_ev = eq[\"per_evidence\"]\n",
    "per_sc = eq[\"per_subclaim\"]\n",
    "\n",
    "print(f\"EVIDENCE GRADING RESULTS\")\n",
    "print(f\"  Graded: {len(per_ev)} evidence items\")\n",
    "print(f\"  Sub-claim summaries: {len(per_sc)}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"  Mode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-evidence grading table\n",
    "grade_rows = []\n",
    "for ev_id, info in sorted(per_ev.items(), key=lambda x: x[1].get(\"evidence_strength\", 0), reverse=True):\n",
    "    # Find matching evidence object for title\n",
    "    ev_obj = next((e for e in evidence if e.id == ev_id), None)\n",
    "    title = ev_obj.title[:45] if ev_obj else \"?\"\n",
    "    \n",
    "    # Get direction for first sub-claim\n",
    "    relevance = info.get(\"relevance\", {})\n",
    "    first_rel = next(iter(relevance.values()), {}) if relevance else {}\n",
    "    \n",
    "    grade_rows.append({\n",
    "        \"id\": ev_id,\n",
    "        \"title\": title,\n",
    "        \"study_type\": info.get(\"study_type\", \"?\"),\n",
    "        \"hierarchy_wt\": info.get(\"hierarchy_weight\", 0),\n",
    "        \"methodology\": info.get(\"methodology_score\", 0),\n",
    "        \"direction\": first_rel.get(\"direction\", \"?\"),\n",
    "        \"relevance\": first_rel.get(\"score\", 0),\n",
    "        \"strength\": info.get(\"evidence_strength\", 0),\n",
    "    })\n",
    "\n",
    "grade_df = pd.DataFrame(grade_rows)\n",
    "print(\"Per-Evidence Grading (sorted by strength):\")\n",
    "display(grade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per sub-claim summary\n",
    "print(\"PER SUB-CLAIM EVIDENCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sc_summary_rows = []\n",
    "for sc_id, info in per_sc.items():\n",
    "    sc_obj = next((sc for sc in state[\"sub_claims\"] if sc.id == sc_id), None)\n",
    "    sc_text = sc_obj.text[:50] if sc_obj else \"?\"\n",
    "    ds = info.get(\"direction_summary\", {})\n",
    "    sc_summary_rows.append({\n",
    "        \"sub_claim\": f\"{sc_id}: {sc_text}\",\n",
    "        \"evidence_count\": info.get(\"evidence_count\", 0),\n",
    "        \"avg_strength\": round(info.get(\"avg_strength\", 0), 3),\n",
    "        \"supports\": ds.get(\"supports\", 0),\n",
    "        \"opposes\": ds.get(\"opposes\", 0),\n",
    "        \"neutral\": ds.get(\"neutral\", 0),\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(sc_summary_rows))\n",
    "\n",
    "# Top evidence per sub-claim\n",
    "for sc_id, info in per_sc.items():\n",
    "    top_ids = info.get(\"top_evidence_ids\", [])\n",
    "    if top_ids:\n",
    "        print(f\"\\n  [{sc_id}] Top evidence: {', '.join(top_ids[:3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a single evidence item's full grading\n",
    "if per_ev:\n",
    "    # Pick the strongest\n",
    "    best_id = max(per_ev, key=lambda k: per_ev[k].get(\"evidence_strength\", 0))\n",
    "    best = per_ev[best_id]\n",
    "    ev_obj = next((e for e in evidence if e.id == best_id), None)\n",
    "    \n",
    "    print(f\"DETAILED GRADING: {best_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    if ev_obj:\n",
    "        print(f\"  Title: {ev_obj.title}\")\n",
    "        print(f\"  Source: {ev_obj.source} | PMID: {ev_obj.pmid or '—'}\")\n",
    "    \n",
    "    print(f\"\\n  Study Type: {best.get('study_type', '?')}\")\n",
    "    print(f\"  Hierarchy Weight: {best.get('hierarchy_weight', 0)}\")\n",
    "    print(f\"  Methodology Score: {best.get('methodology_score', 0)}\")\n",
    "    print(f\"  Evidence Strength: {best.get('evidence_strength', 0)}\")\n",
    "    \n",
    "    # Relevance per sub-claim\n",
    "    rel = best.get(\"relevance\", {})\n",
    "    if rel:\n",
    "        print(f\"\\n  Relevance:\")\n",
    "        for sc_id, r in rel.items():\n",
    "            print(f\"    {sc_id}: direction={r.get('direction', '?')}, score={r.get('score', 0):.2f}\")\n",
    "            if r.get(\"key_finding\"):\n",
    "                print(f\"            finding: {r['key_finding'][:80]}\")\n",
    "    \n",
    "    # GRADE\n",
    "    grade = best.get(\"grade\", {})\n",
    "    if grade:\n",
    "        print(f\"\\n  GRADE Framework:\")\n",
    "        for k, v in grade.items():\n",
    "            print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Claim: \\\"{state['claim']}\\\"\")\n",
    "print(f\"  Sub-claims: {len(state['sub_claims'])}\")\n",
    "print(f\"  Evidence items: {len(state['evidence'])}\")\n",
    "print(f\"  Graded evidence: {len(per_ev)}\")\n",
    "print(f\"  Total cost: ${state['total_cost_usd']:.4f}\")\n",
    "print(f\"  Total duration: {state['total_duration_seconds']:.2f}s\")\n",
    "\n",
    "# Node trace table\n",
    "trace_rows = []\n",
    "for t in state[\"agent_trace\"]:\n",
    "    trace_rows.append({\n",
    "        \"agent\": t.agent,\n",
    "        \"type\": t.node_type,\n",
    "        \"duration_s\": t.duration_seconds,\n",
    "        \"cost_usd\": round(t.cost_usd, 4),\n",
    "        \"tools\": \", \".join(t.tools_called) if t.tools_called else \"—\",\n",
    "        \"reasoning_steps\": t.reasoning_steps,\n",
    "    })\n",
    "\n",
    "print(\"\\nAgent Traces:\")\n",
    "display(pd.DataFrame(trace_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw evidence_quality dict (for inspection)\n",
    "print(\"Raw evidence_quality output (JSON):\")\n",
    "print(json.dumps(state[\"evidence_quality\"], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## Try Another Claim\n",
    "\n",
    "Change the claim below and re-run all cells from Step 1 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick-run: full pipeline in one cell\n",
    "# Uncomment and modify the claim to test a different one\n",
    "\n",
    "# CLAIM2 = \"Vitamin D prevents COVID-19 infection\"\n",
    "# \n",
    "# state2: FactCheckState = {\n",
    "#     \"claim\": CLAIM2, \"pico\": None, \"sub_claims\": [], \"entities\": {},\n",
    "#     \"retrieval_plan\": {}, \"evidence\": [], \"extracted_figures\": [],\n",
    "#     \"evidence_quality\": {}, \"verdict\": \"\", \"confidence\": 0.0,\n",
    "#     \"explanation\": \"\", \"safety_flags\": [], \"is_dangerous\": False,\n",
    "#     \"agent_trace\": [], \"total_cost_usd\": 0.0, \"total_duration_seconds\": 0.0,\n",
    "# }\n",
    "# \n",
    "# state2 = await run_decomposer(state2)\n",
    "# state2 = await run_retrieval_planner(state2)\n",
    "# state2 = await run_evidence_retriever(state2)\n",
    "# state2 = await run_evidence_grader(state2)\n",
    "# \n",
    "# print(f\"Claim: {state2['claim']}\")\n",
    "# print(f\"Sub-claims: {len(state2['sub_claims'])}\")\n",
    "# print(f\"Evidence: {len(state2['evidence'])}\")\n",
    "# print(f\"Cost: ${state2['total_cost_usd']:.4f}\")\n",
    "# print(f\"Duration: {state2['total_duration_seconds']:.1f}s\")\n",
    "# \n",
    "# for sc_id, info in state2['evidence_quality']['per_subclaim'].items():\n",
    "#     ds = info['direction_summary']\n",
    "#     print(f\"  {sc_id}: {info['evidence_count']} items, \"\n",
    "#           f\"strength={info['avg_strength']:.3f}, \"\n",
    "#           f\"supports={ds['supports']}, opposes={ds['opposes']}, neutral={ds['neutral']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Health Claim Checker (Python 3.12)",
   "language": "python",
   "name": "health-claim-checker"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
