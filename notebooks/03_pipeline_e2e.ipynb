{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 03 — Pipeline E2E: Full 6-Node Pipeline\n",
    "\n",
    "Run all 6 nodes of the S4/S6 pipeline on a claim and inspect inputs/outputs at each step.\n",
    "\n",
    "**Nodes covered:**\n",
    "1. **Claim Decomposer** (function) — entities, PICO, sub-claims\n",
    "2. **Retrieval Planner** (ReAct agent) — method selection per sub-claim\n",
    "3. **Evidence Retriever** (ReAct agent) — multi-source search, reranking, dedup\n",
    "4. **Evidence Grader** (ReAct agent) — study type, methodology, relevance, GRADE\n",
    "5. **Verdict Agent** (ReAct agent) — weigh evidence, reconcile conflicts, assign verdicts\n",
    "6. **Safety Checker** (function) — flag dangerous health advice\n",
    "\n",
    "Each step shows the state delta — what the node added to the pipeline state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from src.models import FactCheckState\n",
    "from src.functions.decomposer import run_decomposer\n",
    "from src.functions.safety_checker import run_safety_checker, SAFETY_CATEGORIES\n",
    "from systems.s4_langgraph.agents.retrieval_planner import run_retrieval_planner\n",
    "from systems.s4_langgraph.agents.evidence_retriever import run_evidence_retriever\n",
    "from systems.s4_langgraph.agents.evidence_grader import run_evidence_grader\n",
    "from systems.s4_langgraph.agents.verdict_agent import run_verdict_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: \"The MMR vaccine causes autism in children\"\n",
      "State keys: ['claim', 'pico', 'sub_claims', 'entities', 'retrieval_plan', 'evidence', 'extracted_figures', 'evidence_quality', 'verdict', 'confidence', 'explanation', 'safety_flags', 'is_dangerous', 'agent_trace', 'total_cost_usd', 'total_duration_seconds']\n"
     ]
    }
   ],
   "source": [
    "# Choose a claim to test\n",
    "CLAIM = \"The MMR vaccine causes autism in children\"\n",
    "\n",
    "# Build initial pipeline state\n",
    "state: FactCheckState = {\n",
    "    \"claim\": CLAIM,\n",
    "    \"pico\": None,\n",
    "    \"sub_claims\": [],\n",
    "    \"entities\": {},\n",
    "    \"retrieval_plan\": {},\n",
    "    \"evidence\": [],\n",
    "    \"extracted_figures\": [],\n",
    "    \"evidence_quality\": {},\n",
    "    \"verdict\": \"\",\n",
    "    \"confidence\": 0.0,\n",
    "    \"explanation\": \"\",\n",
    "    \"safety_flags\": [],\n",
    "    \"is_dangerous\": False,\n",
    "    \"agent_trace\": [],\n",
    "    \"total_cost_usd\": 0.0,\n",
    "    \"total_duration_seconds\": 0.0,\n",
    "}\n",
    "\n",
    "print(f'Claim: \"{CLAIM}\"')\n",
    "print(f'State keys: {list(state.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Claim Decomposer (function node)\n",
    "\n",
    "**What it does:** Extracts medical entities via scispaCy NER, extracts PICO elements, decomposes the claim into atomic sub-claims.\n",
    "\n",
    "**Inputs:** `claim` (string)  \n",
    "**Outputs:** `entities`, `pico`, `sub_claims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITIES\n",
      "==================================================\n",
      "  conditions: ['MMR', 'vaccine', 'autism', 'children']\n"
     ]
    }
   ],
   "source": [
    "state = await run_decomposer(state)\n",
    "\n",
    "# --- Entities ---\n",
    "print(\"ENTITIES\")\n",
    "print(\"=\" * 50)\n",
    "entities = state[\"entities\"]\n",
    "for etype, elist in entities.items():\n",
    "    if elist:\n",
    "        print(f\"  {etype}: {elist}\")\n",
    "if not any(v for v in entities.values()):\n",
    "    print(\"  (none detected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PICO EXTRACTION\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Population</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>MMR vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>(null)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outcome</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Element        Value\n",
       "0    Population     children\n",
       "1  Intervention  MMR vaccine\n",
       "2    Comparison       (null)\n",
       "3       Outcome       autism"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- PICO ---\n",
    "print(\"PICO EXTRACTION\")\n",
    "print(\"=\" * 50)\n",
    "pico = state[\"pico\"]\n",
    "if pico:\n",
    "    pico_df = pd.DataFrame([{\n",
    "        \"Element\": el,\n",
    "        \"Value\": getattr(pico, el.lower()) or \"(null)\",\n",
    "    } for el in [\"Population\", \"Intervention\", \"Comparison\", \"Outcome\"]])\n",
    "    display(pico_df)\n",
    "else:\n",
    "    print(\"  (no PICO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB-CLAIMS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>P</th>\n",
       "      <th>I</th>\n",
       "      <th>C</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sc-1</td>\n",
       "      <td>The MMR vaccine causes autism in children</td>\n",
       "      <td>children</td>\n",
       "      <td>MMR vaccine</td>\n",
       "      <td></td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                       text         P            I C  \\\n",
       "0  sc-1  The MMR vaccine causes autism in children  children  MMR vaccine     \n",
       "\n",
       "        O  \n",
       "0  autism  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duration: 4.45s | Cost: $0.0020\n"
     ]
    }
   ],
   "source": [
    "# --- Sub-claims ---\n",
    "print(\"SUB-CLAIMS\")\n",
    "print(\"=\" * 50)\n",
    "sc_rows = []\n",
    "for sc in state[\"sub_claims\"]:\n",
    "    row = {\"id\": sc.id, \"text\": sc.text}\n",
    "    if sc.pico:\n",
    "        row[\"P\"] = sc.pico.population or \"\"\n",
    "        row[\"I\"] = sc.pico.intervention or \"\"\n",
    "        row[\"C\"] = sc.pico.comparison or \"\"\n",
    "        row[\"O\"] = sc.pico.outcome or \"\"\n",
    "    sc_rows.append(row)\n",
    "\n",
    "display(pd.DataFrame(sc_rows))\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "print(f\"\\nDuration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Retrieval Planner (ReAct agent)\n",
    "\n",
    "**What it does:** Examines each sub-claim's characteristics and decides which retrieval methods to use.\n",
    "\n",
    "**Inputs:** `sub_claims`, `entities`, `pico`  \n",
    "**Outputs:** `retrieval_plan` (dict mapping sub-claim ID → list of methods)\n",
    "\n",
    "Uses ReAct with Claude if `ANTHROPIC_API_KEY` is set, otherwise falls back to rule-based keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVAL PLAN\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_claim</th>\n",
       "      <th>methods</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sc-1: The MMR vaccine causes autism in children</td>\n",
       "      <td>pubmed_api, semantic_scholar, cross_encoder</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sub_claim  \\\n",
       "0  sc-1: The MMR vaccine causes autism in children   \n",
       "\n",
       "                                       methods  count  \n",
       "0  pubmed_api, semantic_scholar, cross_encoder      3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mode: ReAct (LLM) | Duration: 17.0s | Cost: $0.0240\n"
     ]
    }
   ],
   "source": [
    "state = await run_retrieval_planner(state)\n",
    "\n",
    "print(\"RETRIEVAL PLAN\")\n",
    "print(\"=\" * 60)\n",
    "plan_rows = []\n",
    "for sc_id, methods in state[\"retrieval_plan\"].items():\n",
    "    sc_text = next((sc.text for sc in state[\"sub_claims\"] if sc.id == sc_id), \"?\")\n",
    "    plan_rows.append({\n",
    "        \"sub_claim\": f\"{sc_id}: {sc_text[:60]}\",\n",
    "        \"methods\": \", \".join(methods),\n",
    "        \"count\": len(methods),\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(plan_rows))\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"\\nMode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Evidence Retriever (ReAct agent)\n",
    "\n",
    "**What it does:** Executes the retrieval plan — searches PubMed, Semantic Scholar, Cochrane, ClinicalTrials.gov, DrugBank. Re-ranks with cross-encoder, deduplicates, links evidence to sub-claims.\n",
    "\n",
    "**Inputs:** `retrieval_plan`, `sub_claims`, `pico`, `entities`  \n",
    "**Outputs:** `evidence` (list of Evidence objects), updated `sub_claims` (with evidence IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 2408.59it/s, Materializing param=classifier.weight]                                      \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVIDENCE RETRIEVED: 8 items\n",
      "============================================================\n",
      "  [sc-1] The MMR vaccine causes autism in children\n",
      "    8 items — pubmed: 5, semantic_scholar: 3\n",
      "\n",
      "Mode: ReAct (LLM) | Duration: 35.01s | Cost: $0.0303\n"
     ]
    }
   ],
   "source": [
    "state = await run_evidence_retriever(state)\n",
    "\n",
    "evidence = state[\"evidence\"]\n",
    "print(f\"EVIDENCE RETRIEVED: {len(evidence)} items\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Per sub-claim summary\n",
    "for sc in state[\"sub_claims\"]:\n",
    "    sc_ev = [e for e in evidence if e.id in sc.evidence]\n",
    "    by_source = {}\n",
    "    for ev in sc_ev:\n",
    "        by_source[ev.source] = by_source.get(ev.source, 0) + 1\n",
    "    source_str = \", \".join(f\"{s}: {c}\" for s, c in sorted(by_source.items())) if by_source else \"none\"\n",
    "    print(f\"  [{sc.id}] {sc.text[:55]}\")\n",
    "    print(f\"    {len(sc_ev)} items — {source_str}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"\\nMode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All evidence items (sorted by cross-encoder quality score):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>study_type</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>title</th>\n",
       "      <th>pmid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ev-pm-30986133</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>systematic_review</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The MMR Vaccine and Autism.</td>\n",
       "      <td>30986133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ev-pm-25898051</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Autism occurrence by MMR vaccine status among ...</td>\n",
       "      <td>25898051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ev-pm-30831578</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Measles, Mumps, Rubella Vaccination and Autism...</td>\n",
       "      <td>30831578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ev-s2-1a718b134e6c</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The “ Wakefield ” Studies : Studies Hypothesiz...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ev-pm-12421889</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A population-based study of measles, mumps, an...</td>\n",
       "      <td>12421889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ev-s2-53f727d55f90</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Confirmatory bias in health decisions: Evidenc...</td>\n",
       "      <td>32057491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ev-s2-5d9a99535640</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Communicating science to the public: MMR vacci...</td>\n",
       "      <td>14604564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ev-pm-36110492</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>systematic_review</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Does Vaccination Increase the Risk of Autism S...</td>\n",
       "      <td>36110492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id            source         study_type  quality_score  \\\n",
       "0      ev-pm-30986133            pubmed  systematic_review            1.0   \n",
       "1      ev-pm-25898051            pubmed            unknown            1.0   \n",
       "2      ev-pm-30831578            pubmed            unknown            1.0   \n",
       "3  ev-s2-1a718b134e6c  semantic_scholar            unknown            1.0   \n",
       "4      ev-pm-12421889            pubmed            unknown            1.0   \n",
       "5  ev-s2-53f727d55f90  semantic_scholar            unknown            1.0   \n",
       "6  ev-s2-5d9a99535640  semantic_scholar            unknown            1.0   \n",
       "7      ev-pm-36110492            pubmed  systematic_review            1.0   \n",
       "\n",
       "                                               title      pmid  \n",
       "0                        The MMR Vaccine and Autism.  30986133  \n",
       "1  Autism occurrence by MMR vaccine status among ...  25898051  \n",
       "2  Measles, Mumps, Rubella Vaccination and Autism...  30831578  \n",
       "3  The “ Wakefield ” Studies : Studies Hypothesiz...            \n",
       "4  A population-based study of measles, mumps, an...  12421889  \n",
       "5  Confirmatory bias in health decisions: Evidenc...  32057491  \n",
       "6  Communicating science to the public: MMR vacci...  14604564  \n",
       "7  Does Vaccination Increase the Risk of Autism S...  36110492  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show all evidence items as a table\n",
    "ev_rows = []\n",
    "for ev in sorted(evidence, key=lambda e: e.quality_score, reverse=True):\n",
    "    ev_rows.append({\n",
    "        \"id\": ev.id,\n",
    "        \"source\": ev.source,\n",
    "        \"study_type\": ev.study_type or \"unknown\",\n",
    "        \"quality_score\": round(ev.quality_score, 3),\n",
    "        \"title\": ev.title[:65],\n",
    "        \"pmid\": ev.pmid or \"\",\n",
    "    })\n",
    "\n",
    "ev_df = pd.DataFrame(ev_rows)\n",
    "print(f\"All evidence items (sorted by cross-encoder quality score):\")\n",
    "display(ev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP EVIDENCE ITEM: ev-pm-30986133\n",
      "============================================================\n",
      "  Source:       pubmed\n",
      "  Study type:   systematic_review\n",
      "  Quality:      1.0000\n",
      "  PMID:         30986133\n",
      "  Title:        The MMR Vaccine and Autism.\n",
      "  URL:          https://pubmed.ncbi.nlm.nih.gov/30986133/\n",
      "\n",
      "  Content (first 500 chars):\n",
      "  Autism is a developmental disability that can cause significant social, communication, and behavioral challenges. A report published in 1998, but subsequently retracted by the journal, suggested that measles, mumps, and rubella (MMR) vaccine causes autism. However, autism is a neurodevelopmental condition that has a strong genetic component with genesis before one year of age, when MMR vaccine is typically administered. Several epidemiologic studies have not found an association between MMR vacc\n"
     ]
    }
   ],
   "source": [
    "# Inspect a single evidence item in detail\n",
    "if evidence:\n",
    "    top_ev = sorted(evidence, key=lambda e: e.quality_score, reverse=True)[0]\n",
    "    print(f\"TOP EVIDENCE ITEM: {top_ev.id}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Source:       {top_ev.source}\")\n",
    "    print(f\"  Study type:   {top_ev.study_type}\")\n",
    "    print(f\"  Quality:      {top_ev.quality_score:.4f}\")\n",
    "    print(f\"  PMID:         {top_ev.pmid or '—'}\")\n",
    "    print(f\"  Title:        {top_ev.title}\")\n",
    "    print(f\"  URL:          {top_ev.url or '—'}\")\n",
    "    print(f\"\\n  Content (first 500 chars):\")\n",
    "    print(f\"  {top_ev.content[:500]}\")\n",
    "else:\n",
    "    print(\"No evidence retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Evidence Grader (ReAct agent)\n",
    "\n",
    "**What it does:** Evaluates each evidence item for study type, methodological quality, relevance to sub-claims, and applies the GRADE framework. Produces per-evidence quality scores and per-subclaim evidence summaries.\n",
    "\n",
    "**Inputs:** `evidence`, `sub_claims`, `extracted_figures`  \n",
    "**Outputs:** `evidence_quality` dict with:\n",
    "- `per_evidence`: study type, hierarchy weight, methodology score, relevance, GRADE, evidence strength\n",
    "- `per_subclaim`: evidence count, avg strength, direction summary, top evidence IDs\n",
    "\n",
    "Uses ReAct with Claude if `ANTHROPIC_API_KEY` is set, otherwise falls back to rule-based (uses existing metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ReAct loop hit max steps (15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVIDENCE GRADING RESULTS\n",
      "  Graded: 4 evidence items\n",
      "  Sub-claim summaries: 1\n",
      "  Mode: ReAct (LLM) | Duration: 84.85s | Cost: $0.2533\n"
     ]
    }
   ],
   "source": [
    "state = await run_evidence_grader(state)\n",
    "\n",
    "eq = state[\"evidence_quality\"]\n",
    "per_ev = eq[\"per_evidence\"]\n",
    "per_sc = eq[\"per_subclaim\"]\n",
    "\n",
    "print(f\"EVIDENCE GRADING RESULTS\")\n",
    "print(f\"  Graded: {len(per_ev)} evidence items\")\n",
    "print(f\"  Sub-claim summaries: {len(per_sc)}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"  Mode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Evidence Grading (sorted by strength):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>study_type</th>\n",
       "      <th>hierarchy_wt</th>\n",
       "      <th>methodology</th>\n",
       "      <th>direction</th>\n",
       "      <th>relevance</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ev-pm-30986133</td>\n",
       "      <td>The MMR Vaccine and Autism.</td>\n",
       "      <td>systematic_review</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>opposes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ev-pm-25898051</td>\n",
       "      <td>Autism occurrence by MMR vaccine status among</td>\n",
       "      <td>cohort</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>opposes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ev-pm-30831578</td>\n",
       "      <td>Measles, Mumps, Rubella Vaccination and Autis</td>\n",
       "      <td>cohort</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>opposes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ev-pm-12421889</td>\n",
       "      <td>A population-based study of measles, mumps, a</td>\n",
       "      <td>cohort</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>opposes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                          title  \\\n",
       "0  ev-pm-30986133                    The MMR Vaccine and Autism.   \n",
       "1  ev-pm-25898051  Autism occurrence by MMR vaccine status among   \n",
       "2  ev-pm-30831578  Measles, Mumps, Rubella Vaccination and Autis   \n",
       "3  ev-pm-12421889  A population-based study of measles, mumps, a   \n",
       "\n",
       "          study_type  hierarchy_wt  methodology direction  relevance  strength  \n",
       "0  systematic_review           0.9          1.0   opposes        1.0      0.96  \n",
       "1             cohort           0.6          1.0   opposes        1.0      0.84  \n",
       "2             cohort           0.6          1.0   opposes        1.0      0.84  \n",
       "3             cohort           0.6          1.0   opposes        1.0      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Per-evidence grading table\n",
    "grade_rows = []\n",
    "for ev_id, info in sorted(per_ev.items(), key=lambda x: x[1].get(\"evidence_strength\", 0), reverse=True):\n",
    "    # Find matching evidence object for title\n",
    "    ev_obj = next((e for e in evidence if e.id == ev_id), None)\n",
    "    title = ev_obj.title[:45] if ev_obj else \"?\"\n",
    "    \n",
    "    # Get direction for first sub-claim\n",
    "    relevance = info.get(\"relevance\", {})\n",
    "    first_rel = next(iter(relevance.values()), {}) if relevance else {}\n",
    "    \n",
    "    grade_rows.append({\n",
    "        \"id\": ev_id,\n",
    "        \"title\": title,\n",
    "        \"study_type\": info.get(\"study_type\", \"?\"),\n",
    "        \"hierarchy_wt\": info.get(\"hierarchy_weight\", 0),\n",
    "        \"methodology\": info.get(\"methodology_score\", 0),\n",
    "        \"direction\": first_rel.get(\"direction\", \"?\"),\n",
    "        \"relevance\": first_rel.get(\"score\", 0),\n",
    "        \"strength\": info.get(\"evidence_strength\", 0),\n",
    "    })\n",
    "\n",
    "grade_df = pd.DataFrame(grade_rows)\n",
    "print(\"Per-Evidence Grading (sorted by strength):\")\n",
    "display(grade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER SUB-CLAIM EVIDENCE SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_claim</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>avg_strength</th>\n",
       "      <th>supports</th>\n",
       "      <th>opposes</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sc-1: The MMR vaccine causes autism in children</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sub_claim  evidence_count  \\\n",
       "0  sc-1: The MMR vaccine causes autism in children               4   \n",
       "\n",
       "   avg_strength  supports  opposes  neutral  \n",
       "0          0.66         0        4        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [sc-1] Top evidence: ev-pm-30986133, ev-pm-25898051, ev-pm-30831578\n"
     ]
    }
   ],
   "source": [
    "# Per sub-claim summary\n",
    "print(\"PER SUB-CLAIM EVIDENCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sc_summary_rows = []\n",
    "for sc_id, info in per_sc.items():\n",
    "    sc_obj = next((sc for sc in state[\"sub_claims\"] if sc.id == sc_id), None)\n",
    "    sc_text = sc_obj.text[:50] if sc_obj else \"?\"\n",
    "    ds = info.get(\"direction_summary\", {})\n",
    "    sc_summary_rows.append({\n",
    "        \"sub_claim\": f\"{sc_id}: {sc_text}\",\n",
    "        \"evidence_count\": info.get(\"evidence_count\", 0),\n",
    "        \"avg_strength\": round(info.get(\"avg_strength\", 0), 3),\n",
    "        \"supports\": ds.get(\"supports\", 0),\n",
    "        \"opposes\": ds.get(\"opposes\", 0),\n",
    "        \"neutral\": ds.get(\"neutral\", 0),\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(sc_summary_rows))\n",
    "\n",
    "# Top evidence per sub-claim\n",
    "for sc_id, info in per_sc.items():\n",
    "    top_ids = info.get(\"top_evidence_ids\", [])\n",
    "    if top_ids:\n",
    "        print(f\"\\n  [{sc_id}] Top evidence: {', '.join(top_ids[:3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED GRADING: ev-pm-30986133\n",
      "============================================================\n",
      "  Title: The MMR Vaccine and Autism.\n",
      "  Source: pubmed | PMID: 30986133\n",
      "\n",
      "  Study Type: systematic_review\n",
      "  Hierarchy Weight: 0.9\n",
      "  Methodology Score: 1.0\n",
      "  Evidence Strength: 0.96\n",
      "\n",
      "  Relevance:\n",
      "    sc-1: direction=opposes, score=1.00\n",
      "            finding: Systematic review states that several epidemiologic studies have not found an as\n",
      "\n",
      "  GRADE Framework:\n",
      "    risk_of_bias: no_serious_concern\n",
      "    inconsistency: no_serious_concern\n",
      "    indirectness: no_serious_concern\n",
      "    imprecision: no_serious_concern\n",
      "    publication_bias: no_serious_concern\n",
      "    overall_quality: high\n"
     ]
    }
   ],
   "source": [
    "# Inspect a single evidence item's full grading\n",
    "if per_ev:\n",
    "    # Pick the strongest\n",
    "    best_id = max(per_ev, key=lambda k: per_ev[k].get(\"evidence_strength\", 0))\n",
    "    best = per_ev[best_id]\n",
    "    ev_obj = next((e for e in evidence if e.id == best_id), None)\n",
    "    \n",
    "    print(f\"DETAILED GRADING: {best_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    if ev_obj:\n",
    "        print(f\"  Title: {ev_obj.title}\")\n",
    "        print(f\"  Source: {ev_obj.source} | PMID: {ev_obj.pmid or '—'}\")\n",
    "    \n",
    "    print(f\"\\n  Study Type: {best.get('study_type', '?')}\")\n",
    "    print(f\"  Hierarchy Weight: {best.get('hierarchy_weight', 0)}\")\n",
    "    print(f\"  Methodology Score: {best.get('methodology_score', 0)}\")\n",
    "    print(f\"  Evidence Strength: {best.get('evidence_strength', 0)}\")\n",
    "    \n",
    "    # Relevance per sub-claim\n",
    "    rel = best.get(\"relevance\", {})\n",
    "    if rel:\n",
    "        print(f\"\\n  Relevance:\")\n",
    "        for sc_id, r in rel.items():\n",
    "            print(f\"    {sc_id}: direction={r.get('direction', '?')}, score={r.get('score', 0):.2f}\")\n",
    "            if r.get(\"key_finding\"):\n",
    "                print(f\"            finding: {r['key_finding'][:80]}\")\n",
    "    \n",
    "    # GRADE\n",
    "    grade = best.get(\"grade\", {})\n",
    "    if grade:\n",
    "        print(f\"\\n  GRADE Framework:\")\n",
    "        for k, v in grade.items():\n",
    "            print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verdict Agent (ReAct agent)\n",
    "\n",
    "**What it does:** Weighs evidence for/against each sub-claim using quality scores from the grader, reconciles conflicting evidence, assigns per-sub-claim verdicts, and synthesizes an overall 9-level verdict.\n",
    "\n",
    "**Inputs:** `evidence`, `evidence_quality`, `sub_claims`  \n",
    "**Outputs:** `verdict`, `confidence`, `explanation`, updated `sub_claims` (with verdicts)\n",
    "\n",
    "Uses ReAct with Claude if `ANTHROPIC_API_KEY` is set, otherwise falls back to rule-based (direction counts + avg strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERDICT AGENT RESULTS\n",
      "============================================================\n",
      "  Overall verdict: REFUTED\n",
      "  Confidence: 0.95\n",
      "  Mode: ReAct (LLM) | Duration: 18.27s | Cost: $0.0303\n"
     ]
    }
   ],
   "source": [
    "state = await run_verdict_agent(state)\n",
    "\n",
    "print(\"VERDICT AGENT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Overall verdict: {state['verdict']}\")\n",
    "print(f\"  Confidence: {state['confidence']:.2f}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "mode = \"ReAct (LLM)\" if trace.reasoning_steps > 0 else \"Rule-based\"\n",
    "print(f\"  Mode: {mode} | Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER SUB-CLAIM VERDICTS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sub_claim</th>\n",
       "      <th>verdict</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sc-1</td>\n",
       "      <td>The MMR vaccine causes autism in children</td>\n",
       "      <td>REFUTED</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                  sub_claim  verdict  confidence\n",
       "0  sc-1  The MMR vaccine causes autism in children  REFUTED        0.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation:\n",
      "  The claim that \"The MMR vaccine causes autism in children\" is directly\n",
      "  contradicted by strong scientific evidence. A systematic review (ev-\n",
      "  pm-30986133) with very high quality (strength=0.96) states that multiple\n",
      "  epidemiologic studies have found no association between MMR vaccine and\n",
      "  autism. This is supported by two large cohort studies: one analyzing US\n",
      "  children (ev-pm-25898051, strength=0.84) showing no link between MMR vaccine\n",
      "  and autism spectrum disorders, and a nationwide cohort study of 657,461\n",
      "  children (ev-pm-30831578, strength=0.84) evaluating MMR vaccine and autism\n",
      "  risk. The evidence is unanimous in opposing this claim with no credible\n",
      "  supporting evidence identified. The scientific consensus based on extensive\n",
      "  epidemiological research clearly refutes any causal link between MMR\n",
      "  vaccination and autism.\n"
     ]
    }
   ],
   "source": [
    "# Per sub-claim verdicts\n",
    "print(\"PER SUB-CLAIM VERDICTS\")\n",
    "print(\"=\" * 60)\n",
    "verdict_rows = []\n",
    "for sc in state[\"sub_claims\"]:\n",
    "    verdict_rows.append({\n",
    "        \"id\": sc.id,\n",
    "        \"sub_claim\": sc.text[:55],\n",
    "        \"verdict\": sc.verdict or \"pending\",\n",
    "        \"confidence\": round(sc.confidence, 3),\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(verdict_rows))\n",
    "\n",
    "# Explanation\n",
    "print(f\"\\nExplanation:\")\n",
    "explanation = state.get(\"explanation\", \"\")\n",
    "if explanation:\n",
    "    # Word wrap\n",
    "    import textwrap\n",
    "    print(textwrap.fill(explanation, width=80, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
    "else:\n",
    "    print(\"  (no explanation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Safety Checker (function node)\n",
    "\n",
    "**What it does:** Scans the claim against 6 safety categories for dangerous health advice. Uses a single LLM call (no reasoning loop), with rule-based keyword matching as fallback.\n",
    "\n",
    "**Inputs:** `claim`, `verdict`, `sub_claims`, `evidence`  \n",
    "**Outputs:** `safety_flags`, `is_dangerous`\n",
    "\n",
    "Safety categories:\n",
    "- `stop_medication` — stopping prescribed medication\n",
    "- `unproven_alternative` — replacing proven treatments\n",
    "- `delay_care` — delaying necessary medical care\n",
    "- `dangerous_dosage` — dangerous dosage recommendations\n",
    "- `anti_vaccination` — anti-vaccination claims\n",
    "- `vulnerable_population` — claims about vulnerable groups + bad verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFETY CHECKER RESULTS\n",
      "============================================================\n",
      "  Safety flags (2):\n",
      "    - anti_vaccination: Anti-vaccination or anti-medical-establishment claims\n",
      "    - vulnerable_population: Claims about vulnerable populations (children, pregnant, elderly)\n",
      "\n",
      "  Is dangerous: False\n",
      "\n",
      "  Duration: 3.48s | Cost: $0.0030\n"
     ]
    }
   ],
   "source": [
    "state = await run_safety_checker(state)\n",
    "\n",
    "print(\"SAFETY CHECKER RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "safety_flags = state.get(\"safety_flags\", [])\n",
    "is_dangerous = state.get(\"is_dangerous\", False)\n",
    "\n",
    "if safety_flags:\n",
    "    print(f\"  Safety flags ({len(safety_flags)}):\")\n",
    "    for flag in safety_flags:\n",
    "        desc = SAFETY_CATEGORIES.get(flag, flag)\n",
    "        print(f\"    - {flag}: {desc}\")\n",
    "else:\n",
    "    print(\"  No safety flags triggered.\")\n",
    "\n",
    "print(f\"\\n  Is dangerous: {is_dangerous}\")\n",
    "\n",
    "trace = state[\"agent_trace\"][-1]\n",
    "print(f\"\\n  Duration: {trace.duration_seconds}s | Cost: ${trace.cost_usd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gil22lair1u",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2za81rwin5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE SUMMARY\n",
      "============================================================\n",
      "  Claim: \"The MMR vaccine causes autism in children\"\n",
      "  Sub-claims: 1\n",
      "  Evidence items: 8\n",
      "  Graded evidence: 4\n",
      "\n",
      "  VERDICT: REFUTED\n",
      "  Confidence: 0.95\n",
      "  Dangerous: False\n",
      "  Safety flags: anti_vaccination, vulnerable_population\n",
      "\n",
      "  Total cost: $0.3428\n",
      "  Total duration: 163.07s\n",
      "\n",
      "Agent Traces:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>type</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>cost_usd</th>\n",
       "      <th>tools</th>\n",
       "      <th>reasoning_steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decomposer</td>\n",
       "      <td>function</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>extract_entities, extract_pico, decompose_claim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retrieval_planner</td>\n",
       "      <td>agent</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>check_guideline_coverage, analyze_claim_charac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evidence_retriever</td>\n",
       "      <td>agent</td>\n",
       "      <td>35.01</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>rerank_evidence, search_pubmed, search_semanti...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence_grader</td>\n",
       "      <td>agent</td>\n",
       "      <td>84.85</td>\n",
       "      <td>0.2533</td>\n",
       "      <td>apply_grade, assess_methodology, classify_stud...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verdict_agent</td>\n",
       "      <td>agent</td>\n",
       "      <td>18.27</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>assign_subclaim_verdict, weigh_evidence, synth...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>safety_checker</td>\n",
       "      <td>function</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>llm_safety_check</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                agent      type  duration_s  cost_usd  \\\n",
       "0          decomposer  function        4.45    0.0020   \n",
       "1   retrieval_planner     agent       17.00    0.0240   \n",
       "2  evidence_retriever     agent       35.01    0.0303   \n",
       "3     evidence_grader     agent       84.85    0.2533   \n",
       "4       verdict_agent     agent       18.27    0.0303   \n",
       "5      safety_checker  function        3.48    0.0030   \n",
       "\n",
       "                                               tools  reasoning_steps  \n",
       "0    extract_entities, extract_pico, decompose_claim                0  \n",
       "1  check_guideline_coverage, analyze_claim_charac...                3  \n",
       "2  rerank_evidence, search_pubmed, search_semanti...                4  \n",
       "3  apply_grade, assess_methodology, classify_stud...               15  \n",
       "4  assign_subclaim_verdict, weigh_evidence, synth...                3  \n",
       "5                                   llm_safety_check                0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Claim: \\\"{state['claim']}\\\"\")\n",
    "print(f\"  Sub-claims: {len(state['sub_claims'])}\")\n",
    "print(f\"  Evidence items: {len(state['evidence'])}\")\n",
    "\n",
    "eq = state.get(\"evidence_quality\", {})\n",
    "print(f\"  Graded evidence: {len(eq.get('per_evidence', {}))}\")\n",
    "print()\n",
    "print(f\"  VERDICT: {state['verdict']}\")\n",
    "print(f\"  Confidence: {state['confidence']:.2f}\")\n",
    "print(f\"  Dangerous: {state['is_dangerous']}\")\n",
    "\n",
    "safety_flags = state.get(\"safety_flags\", [])\n",
    "if safety_flags:\n",
    "    print(f\"  Safety flags: {', '.join(safety_flags)}\")\n",
    "\n",
    "print(f\"\\n  Total cost: ${state['total_cost_usd']:.4f}\")\n",
    "print(f\"  Total duration: {state['total_duration_seconds']:.2f}s\")\n",
    "\n",
    "# Node trace table\n",
    "trace_rows = []\n",
    "for t in state[\"agent_trace\"]:\n",
    "    trace_rows.append({\n",
    "        \"agent\": t.agent,\n",
    "        \"type\": t.node_type,\n",
    "        \"duration_s\": t.duration_seconds,\n",
    "        \"cost_usd\": round(t.cost_usd, 4),\n",
    "        \"tools\": \", \".join(t.tools_called) if t.tools_called else \"—\",\n",
    "        \"reasoning_steps\": t.reasoning_steps,\n",
    "    })\n",
    "\n",
    "print(\"\\nAgent Traces:\")\n",
    "display(pd.DataFrame(trace_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2tft62f26cj",
   "metadata": {},
   "source": [
    "---\n",
    "## Try Another Claim\n",
    "\n",
    "Change the claim below and re-run all cells from Step 1 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8mbxzgftwgy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick-run: full pipeline in one cell\n",
    "# Uncomment and modify the claim to test a different one\n",
    "\n",
    "# CLAIM2 = \"Vitamin D prevents COVID-19 infection\"\n",
    "# \n",
    "# state2: FactCheckState = {\n",
    "#     \"claim\": CLAIM2, \"pico\": None, \"sub_claims\": [], \"entities\": {},\n",
    "#     \"retrieval_plan\": {}, \"evidence\": [], \"extracted_figures\": [],\n",
    "#     \"evidence_quality\": {}, \"verdict\": \"\", \"confidence\": 0.0,\n",
    "#     \"explanation\": \"\", \"safety_flags\": [], \"is_dangerous\": False,\n",
    "#     \"agent_trace\": [], \"total_cost_usd\": 0.0, \"total_duration_seconds\": 0.0,\n",
    "# }\n",
    "# \n",
    "# state2 = await run_decomposer(state2)\n",
    "# state2 = await run_retrieval_planner(state2)\n",
    "# state2 = await run_evidence_retriever(state2)\n",
    "# state2 = await run_evidence_grader(state2)\n",
    "# state2 = await run_verdict_agent(state2)\n",
    "# state2 = await run_safety_checker(state2)\n",
    "# \n",
    "# print(f\"Claim: {state2['claim']}\")\n",
    "# print(f\"VERDICT: {state2['verdict']} (confidence: {state2['confidence']:.2f})\")\n",
    "# print(f\"Dangerous: {state2['is_dangerous']}\")\n",
    "# print(f\"Safety flags: {state2['safety_flags']}\")\n",
    "# print(f\"Cost: ${state2['total_cost_usd']:.4f}\")\n",
    "# print(f\"Duration: {state2['total_duration_seconds']:.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Health Claim Checker (Python 3.12)",
   "language": "python",
   "name": "health-claim-checker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
